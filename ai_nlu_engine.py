import os
from utils import log
import pandas as pd
from keybert import KeyBERT
import json

# ì˜¤í”„ë¼ì¸ í—ˆìš© (ëª¨ë¸ì´ ë¡œì»¬ì— ìˆì„ ë•Œ)
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'
os.environ["TRANSFORMERS_OFFLINE"] = os.getenv("TRANSFORMERS_OFFLINE", "0")

from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import re
import torch

"""
    ë²”ìš© ì œì–´ ì‹œìŠ¤í…œ NLU ì—”ì§„

    ì—­í• :
        - ìŒì„± ì¸ì‹ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”ëœ JSON ëª…ë ¹ìœ¼ë¡œ ë³€í™˜
        - Zero-Shot Classification (AI ê¸°ë°˜ ì˜ë¯¸ ë¶„ë¥˜)
        - íŒ¨í„´ ë§¤ì¹­ (ì •ê·œì‹ ê¸°ë°˜ ë¹ ë¥¸ ì¶”ì¶œ)
"""
class UniversalNluEngine:
    MAX_LENGTH = 128
    """
        NLU ì—”ì§„ ì´ˆê¸°í™”

        ìˆ˜í–‰ ì‘ì—…:
            1. Command Hypotheses ì •ì˜ (AI ë¶„ë¥˜ìš© ê°€ì„¤ ë¬¸ì¥)
            2. XLM-RoBERTa XNLI ëª¨ë¸ ë¡œë“œ (ë‹¤êµ­ì–´ NLI)
            3. KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”

        ì„¤ì •:
            - ëª¨ë¸: XLM-RoBERTa Large + XNLI
            - í‚¤ì›Œë“œ ì¶”ì¶œ: KeyBERT (ë‹¤êµ­ì–´ ì§€ì›)
            - ì¡ìŒ í•„í„°: í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
    """
    def __init__(self):
        self.SCENARIO_TO_FILE = {
            "ì‹œí—˜": "A.wav",
            "ë°©ë¥˜": "B.wav",
        }

        # Command ë ˆë²¨ ê°€ì„¤ ìƒì„¸
        # Command ë ˆë²¨ ê°€ì„¤ ìƒì„¸
        self.COMMAND_HYPOTHESES = {
            # 1) ê²½ë³´ ë°©ì†¡
            "alert.broadcast": {
                "description": "ê²½ë³´êµ­ ë°©ì†¡ ìˆ˜í–‰. ì‹œë‚˜ë¦¬ì˜¤(ì‹œí—˜/ë°©ë¥˜)ì™€ ë³¼ë¥¨ì„ ìŠ¬ë¡¯ìœ¼ë¡œ ì¶”ì¶œ.",
                "examples": [
                    "ê²½ë³´êµ­ ë³¼ë¥¨ 1ë¡œ ì‹œí—˜ ë°©ì†¡ ì‹œì‘",
                    "ê²½ë³´êµ­ ë³¼ë¥¨ ì œì¼ ì‘ê²Œ ì‹œí—˜ ë°©ì†¡ ì‹œì‘",
                    "ê²½ë³´êµ­ ë³¼ë¥¨ 30ìœ¼ë¡œ ë°©ë¥˜ ë°©ì†¡í•´ì¤˜",
                    "ê²½ë³´êµ­ ì‹œí—˜ ë°©ì†¡ ì‹œì‘",
                    "ë°©ë¥˜ ì•ˆë‚´ ë°©ì†¡ ì†¡ì¶œ",
                ],
                "keywords": ["ê²½ë³´", "ë°©ì†¡", "ì•ˆë‚´", "ì¬ìƒ", "ì†¡ì¶œ", "ì‹œí—˜", "í…ŒìŠ¤íŠ¸", "ë°©ë¥˜", "ê²½ë³´êµ­"],
                "slots": {
                    "scenario": ["ì‹œí—˜", "ë°©ë¥˜", "í…ŒìŠ¤íŠ¸"],
                    "volume": list(range(0, 101)),
                    "action": ["ì‹œì‘", "ì •ì§€", "ì¤‘ë‹¨", "ì¼œê¸°", "ë„ê¸°"],
                },
                "slot_patterns": {
                    "scenario": r"(ì‹œí—˜|ë°©ë¥˜|í…ŒìŠ¤íŠ¸)",
                    "volume": r"(?:ë³¼ë¥¨|volume)\s*(\d{1,3})",
                    "action": r"(ì‹œì‘|ì •ì§€|ì¤‘ë‹¨|ì¼œ|êº¼)",
                },
                "defaults": {"station": "ê²½ë³´êµ­", "volume": 10, "action": "ì‹œì‘"}
            },

            # 2) ìˆ˜ìœ„êµ­ ë°ì´í„° í˜¸ì¶œ
            "data.fetch.level": {
                "description": "ìˆ˜ìœ„êµ­ì˜ ìˆ˜ìœ„, ìš°ëŸ‰, ë°°í„°ë¦¬ ì „ì•• ë°ì´í„°ë¥¼ ì¡°íšŒí•´ ì‘ë‹µ.",
                "examples": [
                    "ë¶€ì²œ ìˆ˜ìœ„êµ­ ë°ì´í„° í˜¸ì¶œí•´ì¤˜",
                    "ìˆ˜ìœ„êµ­ ë°ì´í„° ê°€ì ¸ì™€",
                    "ë¶€ì²œ ìˆ˜ìœ„, ìš°ëŸ‰, ë°°í„°ë¦¬ ì „ì•• ì¡°íšŒ",
                    "ìˆ˜ìœ„êµ­ ê°’ ë¶ˆëŸ¬ì™€"
                ],
                "keywords": ["ìˆ˜ìœ„êµ­", "ë°ì´í„°", "í˜¸ì¶œ", "ê°€ì ¸ì™€", "ì¡°íšŒ", "ë¶ˆëŸ¬ì™€", "ìˆ˜ìœ„", "ìš°ëŸ‰", "ë°°í„°ë¦¬"],
                "slots": {
                    "station": ["ìˆ˜ìœ„êµ­", "ìš°ëŸ‰êµ­"],
                    "data_type": ["ìˆ˜ìœ„", "ìš°ëŸ‰", "ë°°í„°ë¦¬ì „ì••", "ì „ì²´"],
                },
                "slot_patterns": {
                    "station": r"(ìˆ˜ìœ„êµ­|ìš°ëŸ‰êµ­)",
                    "data_type": r"(ìˆ˜ìœ„|ìš°ëŸ‰|ë°°í„°ë¦¬\s*ì „ì••)",
                },
                "defaults": {"station": "ìˆ˜ìœ„êµ­", "data_type": "ì „ì²´"}
            },

            # 3) ì¥ë¹„ ì ê²€
            "device.inspect": {
                "description": "ì§€ì • êµ­ì˜ ì¥ë¹„Â·ì„¼ì„œ ìƒíƒœ ì ê²€ í›„ ì´ìƒ ì—¬ë¶€ ë¦¬í¬íŠ¸.",
                "examples": [
                    "ìš¸ì‚° ê²½ë³´êµ­ ì¥ë¹„ ì ê²€í•´ì¤˜",
                    "ë¶€ì²œ ê²½ë³´êµ­ ì ê²€",
                    "ê²½ë³´êµ­ ì¥ë¹„ ìƒíƒœ ì²´í¬",
                    "ì¥ë¹„ ì§„ë‹¨ ì‹¤í–‰"
                ],
                "keywords": ["ì ê²€", "ì§„ë‹¨", "ì²´í¬", "ê²€ì‚¬", "ì¥ë¹„", "ìƒíƒœ", "ê²½ë³´êµ­", "ìˆ˜ìœ„êµ­"],
                "slots": {
                    "station": ["ê²½ë³´êµ­", "ìˆ˜ìœ„êµ­"],
                },
                "slot_patterns": {
                    "station": r"(ê²½ë³´êµ­|ìˆ˜ìœ„êµ­)",
                },
                "defaults": {"station": "ê²½ë³´êµ­"}
            },
        }

        # ì¡ìŒ í‚¤ì›Œë“œ
        self.NOISE = ["ìŒ", "ì–´", "ìœ¼", "ì•„", "ì´ì œ", "ì¢€", "ì•½ê°„", "ê·¸", "ì €", "ë­", "ë­ì‹œê¸°",
                      "ê·¸ê±°", "ì €ê±°", "ì´ê±°", "ìˆì–ì•„", "ìˆì–ì•„ìš”", "ìš”", "ì ê¹", "ë¹¨ë¦¬"]


        log("ğŸ¤– AI ëª¨ë¸ ë¡œë”© ì¤‘...")
        model_dir = r"D:\models\xlmR_xnli"

        # ì˜µì…˜ 2: í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ (KoBERT ê¸°ë°˜)
        # Zero-Shot Classification íŒŒì´í”„ë¼ì¸ ìƒì„±
        self.classifier = pipeline(
            "zero-shot-classification",               # ì‘ì—… íƒ€ì…: ì œë¡œìƒ· ë¶„ë¥˜
            model=AutoModelForSequenceClassification.from_pretrained(
                model_dir,                                 # ëª¨ë¸ ê²½ë¡œ
                local_files_only=True                      # ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš© (ì¸í„°ë„· ì°¨ë‹¨)
            ),
            tokenizer=AutoTokenizer.from_pretrained(
                model_dir,                                 # í† í¬ë‚˜ì´ì € ê²½ë¡œ
                local_files_only=True,                     # ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš©
                use_fast=True                              # Rust ê¸°ë°˜ Fast Tokenizer (2~3ë°° ë¹ ë¦„)
            ),
        )

        # KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”
        # - ëª¨ë¸: DistilUSE (ë‹¤êµ­ì–´ ë¬¸ì¥ ì„ë² ë”©)
        # - ìš©ë„: ì¤‘ìš” í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ
        self.keyword_extractor = KeyBERT('distiluse-base-multilingual-cased-v2')

        log("âœ… AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    """
        ìƒíƒœ ì²´í¬ íƒ€ì… ë¶„ë¥˜
    
        Returns:
            "communication" | "power" | "all"
    """
    def _classify_check_type(self, text: str) -> str:
        hypotheses = {
            "communication": "ì´ ëª…ë ¹ì€ í†µì‹  ì—°ê²° ìƒíƒœë‚˜ ë„¤íŠ¸ì›Œí¬ ìƒíƒœ í™•ì¸ì…ë‹ˆë‹¤.",
            "power": "ì´ ëª…ë ¹ì€ ì „ì› ìƒíƒœë‚˜ ë°°í„°ë¦¬ ì „ì•• í™•ì¸ì…ë‹ˆë‹¤.",
            "all": "ì´ ëª…ë ¹ì€ ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ì¢…í•©ì ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤."
        }

        return self._classify_with_hypotheses(text, hypotheses)

    """
        ë°ì´í„° íƒ€ì… ë¶„ë¥˜
    
        Returns:
            "waterlevel" | "rainfall" | "all"
    """
    def _classify_data_type(self, text: str) -> str:
        hypotheses = {
            "waterlevel": "ì´ ëª…ë ¹ì€ ìˆ˜ìœ„ ì„¼ì„œ ë°ì´í„°ë‚˜ ìˆ˜ìœ„ê³„ ì¸¡ì •ê°’ê³¼ ê´€ë ¨ë©ë‹ˆë‹¤.",
            "rainfall": "ì´ ëª…ë ¹ì€ ê°•ìˆ˜ëŸ‰, ìš°ëŸ‰ ë°ì´í„°ì™€ ê´€ë ¨ë©ë‹ˆë‹¤.",
            "all": "ì´ ëª…ë ¹ì€ ëª¨ë“  ì¢…ë¥˜ì˜ ì„¼ì„œ ë°ì´í„°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤."
        }

        return self._classify_with_hypotheses(text, hypotheses)

    """
        ê³µí†µ ë¶„ë¥˜ í—¬í¼ í•¨ìˆ˜
    
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            hypotheses: {label: hypothesis} ë”•ì…”ë„ˆë¦¬
    
        Returns:
            ìµœê³  ì ìˆ˜ ë ˆì´ë¸”
    """
    def _classify_with_hypotheses(self, text: str, hypotheses: dict) -> str:
        tokenizer = self.classifier.tokenizer
        model = self.classifier.model

        scores = []

        for label, hypothesis in hypotheses.items():
            inputs = tokenizer(
                text,
                hypothesis,
                return_tensors="pt",
                truncation=True,
                max_length=512
            )

            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits[0]
                probs = torch.softmax(logits, dim=0)
                entailment_prob = probs[-1].item()

            scores.append((label, entailment_prob))

        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[0][0]

    def _classify_intent(self, text: str):
        """
        Intent ë¶„ë¥˜: ì‚¬ìš©ì ì…ë ¥ì„ COMMAND_HYPOTHESESì˜ Intentë¡œ ë¶„ë¥˜

        Args:
            text: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸

        Returns:
            str: ë¶„ë¥˜ëœ Intent (ì˜ˆ: "alert.broadcast.start")
        """
        # 1ë‹¨ê³„: í‚¤ì›Œë“œ ê¸°ë°˜ ë¹ ë¥¸ í•„í„°ë§
        candidate_intents = []

        for intent, config in self.COMMAND_HYPOTHESES.items():
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            keyword_match_count = sum(
                1 for keyword in config["keywords"]
                if keyword in text.lower()
            )

            if keyword_match_count > 0:
                candidate_intents.append(intent)

        # í‚¤ì›Œë“œ ë§¤ì¹­ëœ Intentê°€ ì—†ìœ¼ë©´ ì „ì²´ Intent ì‚¬ìš©
        if not candidate_intents:
            candidate_intents = list(self.COMMAND_HYPOTHESES.keys())

        log(f"  [Intent í›„ë³´] {candidate_intents}")

        # 2ë‹¨ê³„: NLU ëª¨ë¸ë¡œ ì •í™•í•œ Intent ë¶„ë¥˜
        intent_labels = [
            self.COMMAND_HYPOTHESES[intent]["description"]
            for intent in candidate_intents
        ]

        result = self.classifier(
            text,
            candidate_labels=intent_labels,
            multi_label=False
        )

        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥
        df = pd.DataFrame({
            'Intent': candidate_intents,
            'ì„¤ëª…': intent_labels,
            'ì •í™•ë„': result['scores']
        })
        df = df.sort_values('ì •í™•ë„', ascending=False).reset_index(drop=True)
        print(df)

        # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ Intent ë°˜í™˜
        best_intent_idx = intent_labels.index(result['labels'][0])
        selected_intent = candidate_intents[best_intent_idx]

        log(f"  [ì„ íƒëœ Intent] {selected_intent}")
        return selected_intent

    def _extract_slot_with_regex(self, text: str, slot_name: str, pattern: str):
        """
        ì •ê·œì‹ìœ¼ë¡œ ìŠ¬ë¡¯ ì¶”ì¶œ (1ì°¨ ì‹œë„)

        Args:
            text: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            slot_name: ìŠ¬ë¡¯ ì´ë¦„ (ì˜ˆ: "scenario", "volume")
            pattern: ì •ê·œì‹ íŒ¨í„´

        Returns:
            ì¶”ì¶œëœ ê°’ ë˜ëŠ” None
        """
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            value = match.group(1) if match.groups() else match.group(0)
            log(f"    [ì •ê·œì‹ ì„±ê³µ] {slot_name} = {value}")
            return value
        return None

    def _extract_slot_with_nlu(self, text: str, slot_name: str, candidates: list):
        """
        NLU ëª¨ë¸ë¡œ ìŠ¬ë¡¯ ì¶”ì¶œ (2ì°¨ ì‹œë„)

        Args:
            text: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            slot_name: ìŠ¬ë¡¯ ì´ë¦„
            candidates: ê°€ëŠ¥í•œ í›„ë³´ ê°’ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì¶”ì¶œëœ ê°’ ë˜ëŠ” None
        """
        # íŠ¹ìˆ˜ ì²˜ë¦¬: volume ê°™ì€ ìˆ«ì ìŠ¬ë¡¯
        if slot_name == "volume":
            # í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ
            numbers = re.findall(r'\d+', text)
            if numbers:
                value = int(numbers[0])
                # ë²”ìœ„ ì²´í¬ (0~100)
                if 0 <= value <= 100:
                    log(f"    [NLU-ìˆ«ì] {slot_name} = {value}")
                    return value

            # "ì œì¼ ì‘ê²Œ", "ìµœì†Œ", "ìµœëŒ€" ê°™ì€ í‘œí˜„ ì²˜ë¦¬
            if any(kw in text for kw in ["ì œì¼ ì‘ê²Œ", "ìµœì†Œ", "ì‘ê²Œ"]):
                log(f"    [NLU-ì˜ë¯¸] {slot_name} = 1 (ìµœì†Œ)")
                return 1
            if any(kw in text for kw in ["ì œì¼ í¬ê²Œ", "ìµœëŒ€", "í¬ê²Œ"]):
                log(f"    [NLU-ì˜ë¯¸] {slot_name} = 100 (ìµœëŒ€)")
                return 100

            return None

        # ì¼ë°˜ ìŠ¬ë¡¯: NLU ë¶„ë¥˜
        if not candidates or len(candidates) == 0:
            return None

        # í›„ë³´ê°€ 1ê°œë©´ ë°”ë¡œ ë°˜í™˜
        if len(candidates) == 1:
            return candidates[0]

        # ë¬¸ìì—´ í›„ë³´ë§Œ í•„í„°ë§ (NLU ì…ë ¥ìš©)
        str_candidates = [str(c) for c in candidates if isinstance(c, str)]

        if not str_candidates:
            return None

        try:
            result = self.classifier(
                text,
                candidate_labels=str_candidates,
                multi_label=False
            )

            value = result['labels'][0]
            score = result['scores'][0]

            # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ë¬´ì‹œ (threshold: 0.3)
            if score < 0.3:
                log(f"    [NLU-ì‹¤íŒ¨] {slot_name} ì‹ ë¢°ë„ ë‚®ìŒ ({score:.2f})")
                return None

            log(f"    [NLU ì„±ê³µ] {slot_name} = {value} (ì‹ ë¢°ë„: {score:.2f})")
            return value

        except Exception as e:
            log(f"    [NLU-ì˜¤ë¥˜] {slot_name}: {e}")
            return None

    def _extract_slots(self, text: str, intent: str):
        """
        Intentì— ì •ì˜ëœ ëª¨ë“  ìŠ¬ë¡¯ ì¶”ì¶œ

        Args:
            text: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            intent: ë¶„ë¥˜ëœ Intent

        Returns:
            dict: ì¶”ì¶œëœ ìŠ¬ë¡¯ë“¤ {slot_name: value}
        """
        config = self.COMMAND_HYPOTHESES[intent]
        slots = {}

        log(f"  [ìŠ¬ë¡¯ ì¶”ì¶œ ì‹œì‘] Intent: {intent}")

        # ê° ìŠ¬ë¡¯ë³„ë¡œ ì¶”ì¶œ ì‹œë„
        for slot_name, candidates in config.get("slots", {}).items():
            log(f"  [{slot_name}] ì¶”ì¶œ ì‹œë„...")

            # 1ì°¨: ì •ê·œì‹ ì‹œë„
            pattern = config.get("slot_patterns", {}).get(slot_name)
            if pattern:
                value = self._extract_slot_with_regex(text, slot_name, pattern)
                if value is not None:
                    slots[slot_name] = value
                    continue

            # 2ì°¨: NLU ì‹œë„
            value = self._extract_slot_with_nlu(text, slot_name, candidates)
            if value is not None:
                slots[slot_name] = value

        # ê¸°ë³¸ê°’ ì ìš©
        defaults = config.get("defaults", {})
        for key, default_value in defaults.items():
            if key not in slots:
                slots[key] = default_value
                log(f"    [ê¸°ë³¸ê°’ ì ìš©] {key} = {default_value}")

        log(f"  [ìµœì¢… ìŠ¬ë¡¯] {slots}")
        return slots

    # 0. ì¡ìŒ ì²˜ë¦¬
    def _preprocess(self, text: str) -> str:
        """ì¡ìŒ ì œê±° ì „ì²˜ë¦¬"""
        t = text.strip().lower()
        for n in self.NOISE:
            t = re.sub(r'\b' + re.escape(n) + r'\b', ' ', t)
            t = t.replace(' ' + n + ' ', ' ').replace(' ' + n, ' ').replace(n + ' ', ' ')
        return re.sub(r'\s+', ' ', t).strip()

    def _extract_keywords(self, text: str, top_n: int = 5):
        text = self._preprocess(text)
        log(f"   ì¡ìŒ ì œê±° í›„ : {text}")
        keywords = self.keyword_extractor.extract_keywords(
            text,
            keyphrase_ngram_range=(1, 1), # ì—°ì†ëœ ë‹¨ì–´ n ë¶€í„° mê°œ ê¹Œì§€ í•˜ë‚˜ì˜ í‚¤ì›Œë“œ í›„ë³´ë¡œ ë³¸ë‹¤.
            stop_words=self.NOISE,        # ë¶ˆí•„ìš”í•œ ë¶ˆìš©ì–´ ì œê±°
            top_n=top_n                   # ëª‡ ê°œì˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ì§€ ê²°ì •
        )
        print(f"í‚¤ì›Œë“œ ì¤‘ìš”ë„ ë¶„ì„ : {[(kw, score) for kw, score in keywords]}")
        # [('ê²½ë³´êµ­', 0.74), ('í¬íŠ¸', 0.68)...] â†’ ['ê²½ë³´êµ­', 'í¬íŠ¸', ...]
        return [kw for kw, score in keywords]

    """
        ëª…ë ¹ íƒ€ì… ë¶„ë¥˜ (ìˆœìˆ˜ AI)

        Args:
            text: ì‚¬ìš©ì ì…ë ¥ (ì˜ˆ: "ì„œë²„ì—ì„œ ìƒìˆ˜ë„ ë°ì´í„° í˜¸ì¶œí•´ì¤˜")

        Returns:
            (command_type, confidence_score)
            ì˜ˆ: ("data.fetch", 0.92)
    """
    def _classify_command(self, text: str) -> tuple:
        log("=" * 60)
        log(f"[1ë‹¨ê³„: Command ë¶„ë¥˜] ì…ë ¥: {text}")
        log("=" * 60)

        # Tokenizerì™€ Model
        tokenizer = self.classifier.tokenizer
        model = self.classifier.model

        scores = []

        for cmd_type, hypothesis_data in self.COMMAND_HYPOTHESES.items():
            # Hypothesis ìƒì„± (description + examples)
            hypothesis = hypothesis_data["description"]

            # ì˜ˆì‹œ ì¶”ê°€
            examples = hypothesis_data["examples"]
            if examples:
                hypothesis += f" ì˜ˆì‹œ: {' '.join(examples)}"

            # NLI ì¶”ë¡ 
            inputs = tokenizer(
                text,
                hypothesis,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True
            )

            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits[0]

                # Softmaxë¡œ í™•ë¥  ë³€í™˜
                probs = torch.softmax(logits, dim=0)
                entailment_prob = probs[-1].item()  # XNLI: ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ê°€ entailment

            scores.append((cmd_type, entailment_prob))

            log(f"  {cmd_type:20s} â†’ {entailment_prob:.4f}")

        # ìµœê³  ì ìˆ˜ ì„ íƒ
        scores.sort(key=lambda x: x[1], reverse=True)
        best_cmd, best_score = scores[0]

        log(f"  âœ… ì¶”ë¡  : {best_cmd} (í™•ì‹ ë„: {best_score:.2%})")
        log("=" * 60)

        return best_cmd, best_score

    """
        ëª…ë ¹ ëŒ€ìƒ ë²”ìœ„ ë¶„ë¥˜ (ë¡œì»¬/ì›ê²©)

        Args:
            text: "ì„œë²„ì—ì„œ ìƒìˆ˜ë„ ë°ì´í„° í˜¸ì¶œ"

        Returns:
            "local" or "remote"
    """
    def _classify_target_scope(self, text: str) -> str:
        log("=" * 60)
        log(f"[2ë‹¨ê³„: Target Scope ë¶„ë¥˜]")
        log("=" * 60)

        # Hypothesis ì •ì˜
        target_hypotheses = {
            "local": "ì´ ëª…ë ¹ì€ í˜„ì¬ ê¸°ê¸°ì—ì„œ ì§ì ‘ ì‹¤í–‰í•˜ëŠ” ë¡œì»¬ ì‘ì—…ì…ë‹ˆë‹¤.",
            "remote": "ì´ ëª…ë ¹ì€ ì›ê²© ì„œë²„ë‚˜ ë‹¤ë¥¸ ì¥ì†Œì˜ ì‹œìŠ¤í…œì— ìš”ì²­í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤."
        }

        tokenizer = self.classifier.tokenizer
        model = self.classifier.model

        scores = []

        for scope, hypothesis in target_hypotheses.items():
            inputs = tokenizer(
                text,                                  # premise: ì‚¬ìš©ì ì›ë¬¸ í…ìŠ¤íŠ¸
                hypothesis,                            # hypothesis: ë ˆì´ë¸”ë³„ ê°€ì„¤ ë¬¸ì¥
                return_tensors="pt",                   # PyTorch í…ì„œë¡œ ë°˜í™˜
                truncation=True,                       # ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ë©´ ì˜ë¼ë‚¸ë‹¤
                max_length=self.MAX_LENGTH             # ë¬¸ì¥ìŒ ì´ ê¸¸ì´ ìƒí•œ(ëª¨ë¸ í•œê³„ì— ë§ì¶¤)
            )

            with torch.no_grad():                      # ì¶”ë¡  ëª¨ë“œ(gradient ë¹„ê³„ì‚°)ë¡œ ë©”ëª¨ë¦¬/ì†ë„ ì ˆì•½
                outputs = model(**inputs)              # NLI ëª¨ë¸ ì „í–¥ íŒ¨ìŠ¤(forward) í˜¸ì¶œ
                logits = outputs.logits[0]             # ë°°ì¹˜ ì²« í•­ëª©ì˜ ë¡œì§“ ë²¡í„° ì·¨ë“
                probs = torch.softmax(logits, dim=0)   # ë¡œì§“ì„ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í™•ë¥ ë¡œ ë³€í™˜
                entailment_prob = probs[-1].item()     # ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ë¥¼ í•¨ì˜(entailment)ë¡œ ê°€ì •í•´ í™•ë¥  ì¶”ì¶œ

            scores.append((scope, entailment_prob))
            log(f"  {scope:10s} â†’ {entailment_prob:.4f}")

        scores.sort(key=lambda x: x[1], reverse=True)
        best_scope = scores[0][0]

        log(f"  í˜„ì¥/ì„œë²„ : {best_scope}\n")

        return best_scope

    """
        ì¥ì†Œëª… ì¶”ì¶œ (í•˜ì´ë¸Œë¦¬ë“œ)
    
        Args:
            text: "ì„œë²„ì—ì„œ ì†Œì–‘ê°•ëŒ ë°ì´í„° í˜¸ì¶œ"
    
        Returns:
            "ì†Œì–‘ê°•ëŒ" or None
    """
    def _extract_location(self, text: str) -> str:
        log("[ì¥ì†Œ ì¶”ì¶œ]")

        # ========================================
        # Step 1: íŒ¨í„´ ê¸°ë°˜ ì¶”ì¶œ (ë¹ ë¥¸ ì²˜ë¦¬)
        # ========================================

        # íŒ¨í„´ 1: "ì„œë²„ì—ì„œ [ì¥ì†Œ] ..."
        pattern1 = r'ì„œë²„ì—ì„œ\s+(\S+?)\s+(?:ë°ì´í„°|ê²½ë³´|ì¥ë¹„|ìƒíƒœ)'
        match = re.search(pattern1, text)

        if match:
            candidate = match.group(1)
            log(f"  íŒ¨í„´ í›„ë³´: {candidate}")

            # 2. ì‹œì„¤ í‚¤ì›Œë“œ: ëŒ, êµ, êµ­ ë“± ì ‘ë¯¸ì‚¬ ê²€ìƒ‰
            if any(kw in candidate for kw in ["ëŒ", "êµ", "êµ­"]):
                log(f"  âœ… ì‹œì„¤ëª… í™•ì •: {candidate}")
                return candidate

            # ì• ë§¤í•œ ê²½ìš° AI ê²€ì¦ìœ¼ë¡œ ë„˜ê¹€
            else:
                log(f"  âš ï¸ ì• ë§¤í•¨ â†’ AI ê²€ì¦ í•„ìš”")
                return self._verify_location_with_ai(text, candidate)

        # 3. AI ê²€ì¦: ì• ë§¤í•œ ê²½ìš° NLI ëª¨ë¸ë¡œ í™•ì¸
        facility_patterns = [
            r'(\S+ëŒ \S+êµ \S+êµ­)',
            r'(\S+ëŒ \S+êµ)',
            r'(\S+ëŒ \S+êµ­)',
            r'(\S+êµ \S+êµ­)',
            r'(\S+ëŒ)',
            r'(\S+êµ)',
            r'(\S+êµ­)',
        ]

        for pattern in facility_patterns:
            match = re.search(pattern, text)
            if match:
                location = match.group(1)
                log(f"  âœ… ì‹œì„¤ íŒ¨í„´: {location}")
                return location

        # 4. AI ë¶„ë¥˜: ë“±ë¡ëœ ì£¼ìš” ì¥ì†Œ ì¤‘ ì„ íƒ (10ê°œ ì´í•˜ ê¶Œì¥)
        common_locations = [
            "ìˆ˜ìœ„ìš°ëŸ‰êµ­",
            "ìˆ˜ìœ„êµ­",
            "ìš°ëŸ‰êµ­",
            "ê²½ë³´êµ­",
            "í†µì‹ ì‹¤"
        ]

        return self._classify_location_ai(text, common_locations)

    """
        í›„ë³´ ì¥ì†Œëª… AI ê²€ì¦

        Args:
            text: ì „ì²´ ë¬¸ì¥
            candidate: ì¶”ì¶œëœ í›„ë³´ (ì˜ˆ: "ìƒìˆ˜ë„")

        Returns:
            ê²€ì¦ëœ ì¥ì†Œëª… or None
    """
    def _verify_location_with_ai(self, text: str, candidate: str) -> str:
        log(f"  [AI ê²€ì¦] í›„ë³´: {candidate}")

        hypothesis = f"ì´ ë¬¸ì¥ì—ì„œ '{candidate}'ëŠ” íŠ¹ì • ì¥ì†Œë‚˜ ì‹œì„¤ì„ ê°€ë¦¬í‚µë‹ˆë‹¤."

        tokenizer = self.classifier.tokenizer
        model = self.classifier.model

        inputs = tokenizer(
            text,                                   # premise: ì‚¬ìš©ì ì›ë¬¸ í…ìŠ¤íŠ¸
            hypothesis,                             # hypothesis: ë ˆì´ë¸”ë³„ ê°€ì„¤ ë¬¸ì¥
            return_tensors="pt",                    # PyTorch í…ì„œë¡œ ë°˜í™˜
            truncation=True,                        # ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ë©´ ì˜ë¼ë‚¸ë‹¤
            max_length=self.MAX_LENGTH              # ë¬¸ì¥ìŒ ì´ ê¸¸ì´ ìƒí•œ(ëª¨ë¸ í•œê³„ì— ë§ì¶¤)
        )

        with torch.no_grad():                       # ì¶”ë¡  ëª¨ë“œ(gradient ë¹„ê³„ì‚°)ë¡œ ë©”ëª¨ë¦¬/ì†ë„ ì ˆì•½
            outputs = model(**inputs)               # NLI ëª¨ë¸ ì „í–¥ íŒ¨ìŠ¤(forward) í˜¸ì¶œ
            logits = outputs.logits[0]              # ë°°ì¹˜ ì²« í•­ëª©ì˜ ë¡œì§“ ë²¡í„° ì·¨ë“
            probs = torch.softmax(logits, dim=0)    # ë¡œì§“ì„ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í™•ë¥ ë¡œ ë³€í™˜
            entailment_prob = probs[-1].item()      # ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ë¥¼ í•¨ì˜(entailment)ë¡œ ê°€ì •í•´ í™•ë¥  ì¶”ì¶œ

        log(f"    í™•ë¥ : {entailment_prob:.4f}")

        # ì„ê³„ê°’ ì„¤ì •
        if entailment_prob > 0.7:
            log(f"  âœ… ê²€ì¦ ì„±ê³µ: {candidate}")
            return candidate
        else:
            log(f"  âŒ ê²€ì¦ ì‹¤íŒ¨")
            return None

    """
        ë“±ë¡ëœ ì¥ì†Œ ì¤‘ AI ë¶„ë¥˜

        Args:
            text: ì…ë ¥ ë¬¸ì¥
            candidates: í›„ë³´ ì¥ì†Œ ëª©ë¡ (ì‘ì€ ë¦¬ìŠ¤íŠ¸ë§Œ!)

        Returns:
            ê°€ì¥ ì í•©í•œ ì¥ì†Œ or None
    """
    def _classify_location_ai(self, text: str, candidates: list) -> str:
        log(f"  [AI ë¶„ë¥˜] í›„ë³´: {candidates}")

        tokenizer = self.classifier.tokenizer
        model = self.classifier.model

        # "ì¥ì†Œ ì—†ìŒ" ì¼€ì´ìŠ¤ ì¶”ê°€
        candidates_with_none = candidates + ["ì—†ìŒ"]

        hypotheses = {
            loc: f"ì´ ë¬¸ì¥ì€ {loc}ì™€ ê´€ë ¨ëœ ëª…ë ¹ì…ë‹ˆë‹¤."
            for loc in candidates
        }
        hypotheses["ì—†ìŒ"] = "ì´ ë¬¸ì¥ì—ëŠ” íŠ¹ì • ì¥ì†Œê°€ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        scores = []

        for loc, hypothesis in hypotheses.items():
            # hypotheses ë”•ì…”ë„ˆë¦¬ì—ì„œ (ë ˆì´ë¸”ì½”ë“œ/í‚¤, ê°€ì„¤ë¬¸) ìŒì„ í•œ ê°œì”© ê°€ì ¸ì˜¨ë‹¤. ì˜ˆ: ("ALERT", "ì´ ë¬¸ì¥ì€ ê²½ë³´...")
            inputs = tokenizer(
                text,                         # premise: ì‚¬ìš©ì ì›ë¬¸ í…ìŠ¤íŠ¸
                hypothesis,                   # hypothesis: ë ˆì´ë¸”ë³„ ê°€ì„¤ ë¬¸ì¥
                return_tensors="pt",          # PyTorch í…ì„œë¡œ ë°˜í™˜
                truncation=True,              # ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ë©´ ì˜ë¼ë‚¸ë‹¤
                max_length=self.MAX_LENGTH    # ë¬¸ì¥ìŒ ì´ ê¸¸ì´ ìƒí•œ(ëª¨ë¸ í•œê³„ì— ë§ì¶¤)
            )

            with torch.no_grad():                      # ì¶”ë¡  ëª¨ë“œ(gradient ë¹„ê³„ì‚°)ë¡œ ë©”ëª¨ë¦¬/ì†ë„ ì ˆì•½
                outputs = model(**inputs)              # NLI ëª¨ë¸ ì „í–¥ íŒ¨ìŠ¤(forward) í˜¸ì¶œ
                logits = outputs.logits[0]             # ë°°ì¹˜ ì²« í•­ëª©ì˜ ë¡œì§“ ë²¡í„° ì·¨ë“
                probs = torch.softmax(logits, dim=0)   # ë¡œì§“ì„ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í™•ë¥ ë¡œ ë³€í™˜
                entailment_prob = probs[-1].item()     # ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ë¥¼ í•¨ì˜(entailment)ë¡œ ê°€ì •í•´ í™•ë¥  ì¶”ì¶œ

            scores.append((loc, entailment_prob))
            log(f"    {loc:15s} â†’ {entailment_prob:.4f}")

        scores.sort(key=lambda x: x[1], reverse=True)
        best_location = scores[0][0]

        if best_location == "ì—†ìŒ":
            log("  âœ… ì¶”ë¡  ê²°ê³¼: ì¥ì†Œ ì—†ìŒ")
            return None
        else:
            log(f"  âœ… ì¶”ë¡  ê²°ê³¼: {best_location}")
            return best_location

    """
                í…ìŠ¤íŠ¸ ë¶„ì„ - Intent & Slot ê¸°ë°˜ íŒŒì‹±

                Args:
                    text: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸

                Returns:
                    dict: íŒŒì‹± ê²°ê³¼ {intent, slots, ...}
            """

    def parse_text(self, text: str):

        if not text or not text.strip():
            return {"error": "ì…ë ¥ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."}

        log(f"[ê°ì§€í•œ í…ìŠ¤íŠ¸] {text}")
        log("=" * 60)

        # ì „ì²˜ë¦¬
        text = self._preprocess(text)

        # í‚¤ì›Œë“œ ì¶”ì¶œ (ë””ë²„ê¹…ìš©)
        keywords = self._extract_keywords(text)
        log(f"  [í‚¤ì›Œë“œ] {keywords}")

        # 1ë‹¨ê³„: Intent ë¶„ë¥˜
        try:
            intent = self._classify_intent(text)
        except Exception as e:
            log(f"  [Intent ë¶„ë¥˜ ì‹¤íŒ¨] {e}")
            return {"error": "ëª…ë ¹ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}

        # 2ë‹¨ê³„: Slot ì¶”ì¶œ
        try:
            slots = self._extract_slots(text, intent)
        except Exception as e:
            log(f"  [Slot ì¶”ì¶œ ì‹¤íŒ¨] {e}")
            return {"error": "íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}

        # 3ë‹¨ê³„: ìµœì¢… ê²°ê³¼ êµ¬ì„±
        result = {
            "intent": intent,
            "slots": slots,
        }

        # íŠ¹ìˆ˜ ì²˜ë¦¬: ë°©ì†¡ ëª…ë ¹ì˜ ê²½ìš° íŒŒì¼ ë§¤í•‘
        if intent == "alert.broadcast.start" and "scenario" in slots:
            scenario = slots["scenario"]
            if scenario in self.SCENARIO_TO_FILE:
                result["file"] = self.SCENARIO_TO_FILE[scenario]
                log(f"  [íŒŒì¼ ë§¤í•‘] {scenario} â†’ {result['file']}")

        log("=" * 60)
        return result

